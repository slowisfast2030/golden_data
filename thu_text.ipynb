{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from zhconv import convert\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入20220821所有数据\n",
    "all_data = pd.read_csv('../data/all_sample_20220821_spark.csv').drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示所有的列\n",
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意一下，这里的列名和sample pipeline中的列名有一点大小写区别，问题不大\n",
    "cv_columns = ['cv_id', 'currentPosition', 'desiredPosition', 'industry', 'desiredIndustry', 'majorName', 'skills', 'eduTracks', 'jobTracks', 'projectTracks']\n",
    "jd_columns = ['jd_id', 'title', 'category_name', 'tags', 'description', 'requirement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_data = all_data[jd_columns]\n",
    "jd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = all_data[cv_columns]\n",
    "cv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jd可以做3个向量\n",
    "* title + category_name + tags\n",
    "* description\n",
    "* requirement\n",
    "\n",
    "cv可以做4个向量：\n",
    "* currentPosition + desiredPosition\n",
    "* skills\n",
    "* jobTracks\n",
    "* projectTracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量1：title + category_name + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1：缺失值填充\n",
    "\n",
    "jd_data['title'].fillna('', inplace=True)\n",
    "jd_data['category_name'].fillna('', inplace=True)\n",
    "jd_data['tags'].fillna('[]', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2：定义新列，将title + category_name合并\n",
    "\n",
    "col_name1 = 'title'\n",
    "col_name2 = 'category_name'\n",
    "\n",
    "def col_merge_str_fun(series):\n",
    "    '''\n",
    "    适用于文本列合并\n",
    "    '''\n",
    "    return series[col_name1] + ' ' + series[col_name2]\n",
    "\n",
    "jd_data['title_category'] = jd_data.apply(col_merge_str_fun, axis=1)\n",
    "jd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3：将新列title_category通过jieba分词处理成列表\n",
    "\n",
    "col_name = 'title_category'\n",
    "\n",
    "def col_jieba_fun(series):\n",
    "    col = series[col_name]\n",
    "\n",
    "    # 字符串变列表\n",
    "    if col.startswith(\"[\") and col.endswith(\"]\"):\n",
    "        col = json.loads(col)\n",
    "    else:\n",
    "        col = re.split(\",|，|/| \", col)\n",
    "\n",
    "    # 对于中文，进入jieba前不需要添加空格\n",
    "    # 不过，如果是中英文混合，就必须空格了\n",
    "    col_str = \" \".join(col)\n",
    "    col_list = jieba.lcut(col_str, cut_all=False)\n",
    "    return col_list\n",
    "\n",
    "jd_data['title_category_jieba'] = jd_data.apply(col_jieba_fun, axis=1)\n",
    "jd_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4：将tags列通过jieba分词处理成列表\n",
    "\n",
    "col_name = 'tags'\n",
    "\n",
    "def col_jieba_fun(series):\n",
    "    col = series[col_name]\n",
    "\n",
    "    # 字符串变列表\n",
    "    if col.startswith(\"[\") and col.endswith(\"]\"):\n",
    "        col = json.loads(col)\n",
    "    else:\n",
    "        col = re.split(\",|，|/| \", col)\n",
    "\n",
    "    # 对于中文，进入jieba前不需要添加空格\n",
    "    # 不过，如果是中英文混合，就必须空格了\n",
    "    col_str = \" \".join(col)\n",
    "    col_list = jieba.lcut(col_str, cut_all=False)\n",
    "    return col_list\n",
    "\n",
    "jd_data['tags_jieba'] = jd_data.apply(col_jieba_fun, axis=1)\n",
    "jd_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step5：将title_category_jieba和tags_jieba两个列表合并\n",
    "\n",
    "col_name1 = 'title_category_jieba'\n",
    "col_name2 = 'tags_jieba'\n",
    "\n",
    "def col_merge_list_fun(series):\n",
    "    '''\n",
    "    适用于列表列合并\n",
    "    '''\n",
    "    return series[col_name1] + series[col_name2]\n",
    "\n",
    "jd_data['title_category_tags_jieba'] = jd_data.apply(col_merge_list_fun, axis=1)\n",
    "jd_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step6：过滤title_category_tags_jieba, 得到以空格分割的字符串\n",
    "\n",
    "col_name = 'title_category_tags_jieba'\n",
    "def col_jieba_filter_fun(series):\n",
    "\n",
    "    col_list_filter = []\n",
    "    # 得到tag列表\n",
    "    col_list = series[col_name]\n",
    "    # print(tags_list[0])\n",
    "\n",
    "    pun_masks_english = [\",\", \".\", \"/\", \"[\", \"]\", \"{\", \"}\", \"(\", \")\", \":\", \"*\", \"#\", \"!\", \" \", \"\\\"\", \"\\\\\"]\n",
    "    pun_masks_chinese = [\"，\", \"。\", \"、\", \"（\", \"）\", \"：\", \"！\", \"”\", \"“\"]\n",
    "    pun_masks = pun_masks_english + pun_masks_chinese\n",
    "\n",
    "    # 过滤\n",
    "    for tag in col_list:\n",
    "        # 转中文简体\n",
    "        tag = convert(tag, \"zh-hans\")\n",
    "        # 转英文小写\n",
    "        tag = tag.lower()\n",
    "\n",
    "        # 过滤数字\n",
    "        if tag.isdigit():\n",
    "            continue\n",
    "        \n",
    "        # 过滤单个字符\n",
    "        if len(tag) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # 过滤标点\n",
    "        flag = 1\n",
    "        for pun in pun_masks:\n",
    "            if pun in tag:\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 1:\n",
    "            col_list_filter.append(tag)\n",
    "    return \" \".join(col_list_filter)\n",
    "\n",
    "jd_data['title_category_tags_jieba_filter'] = jd_data.apply(col_jieba_filter_fun, axis=1)\n",
    "jd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step7：将title_category_tags_jieba_filter转成tfidf向量\n",
    "\n",
    "def get_tfidf(df, col_name):\n",
    "    # col_name必须是以空格分割的字符串\n",
    "    text = df[col_name]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # 返回的是nparray\n",
    "    vector = vectorizer.fit_transform(text)\n",
    "    return pd.DataFrame(vector.toarray()), vectorizer\n",
    "\n",
    "tfidf, vectorizer = get_tfidf(jd_data, 'title_category_tags_jieba_filter')\n",
    "tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step8：将tfidf降维到n维\n",
    "\n",
    "def get_tfidf_pca(tfidf, n=20):\n",
    "    pca = PCA(n_components=n)\n",
    "    tfidf_pca = pca.fit_transform(tfidf)\n",
    "    tfidf_pca = pd.DataFrame(tfidf_pca)\n",
    "    return tfidf_pca\n",
    "\n",
    "tfidf_pca = get_tfidf_pca(tfidf, 10)\n",
    "tfidf_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量2：description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1：缺失值填充\n",
    "\n",
    "jd_data['description'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2：将desciption列通过jieba分词处理成列表\n",
    "\n",
    "col_name = 'description'\n",
    "\n",
    "def col_jieba_fun(series):\n",
    "    col = series[col_name]\n",
    "\n",
    "    # 字符串变列表\n",
    "    if col.startswith(\"[\") and col.endswith(\"]\"):\n",
    "        col = json.loads(col)\n",
    "    else:\n",
    "        col = re.split(\",|，|/| \", col)\n",
    "\n",
    "    # 对于中文，进入jieba前不需要添加空格\n",
    "    # 不过，如果是中英文混合，就必须空格了\n",
    "    col_str = \" \".join(col)\n",
    "    col_list = jieba.lcut(col_str, cut_all=False)\n",
    "    return col_list\n",
    "\n",
    "jd_data['description_jieba'] = jd_data.apply(col_jieba_fun, axis=1)\n",
    "jd_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3：过滤description_jieba，得到以空格分割的字符串\n",
    "\n",
    "col_name = 'description_jieba'\n",
    "\n",
    "def col_jieba_filter_fun(series):\n",
    "\n",
    "    col_list_filter = []\n",
    "    # 得到tag列表\n",
    "    col_list = series[col_name]\n",
    "    # print(tags_list[0])\n",
    "\n",
    "    pun_masks_english = [\",\", \".\", \"/\", \"[\", \"]\", \"{\", \"}\", \"(\", \")\", \":\", \"*\", \"#\", \"!\", \" \", \"\\\"\", \"\\\\\"]\n",
    "    pun_masks_chinese = [\"，\", \"。\", \"、\", \"（\", \"）\", \"：\", \"！\", \"”\", \"“\"]\n",
    "    pun_masks = pun_masks_english + pun_masks_chinese\n",
    "\n",
    "    # 过滤\n",
    "    for tag in col_list:\n",
    "        # 转中文简体\n",
    "        tag = convert(tag, \"zh-hans\")\n",
    "        # 转英文小写\n",
    "        tag = tag.lower()\n",
    "\n",
    "        # 过滤数字\n",
    "        if tag.isdigit():\n",
    "            continue\n",
    "        \n",
    "        # 过滤单个字符\n",
    "        if len(tag) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # 过滤标点\n",
    "        flag = 1\n",
    "        for pun in pun_masks:\n",
    "            if pun in tag:\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 1:\n",
    "            col_list_filter.append(tag)\n",
    "    return \" \".join(col_list_filter)\n",
    "\n",
    "jd_data['description_jieba_filter'] = jd_data.apply(col_jieba_filter_fun, axis=1)\n",
    "jd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4：将description_jieba_filter转成tfidf向量\n",
    "\n",
    "def get_tfidf(df, col_name):\n",
    "    # col_name必须是以空格分割的字符串\n",
    "    text = df[col_name]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # 返回的是nparray\n",
    "    vector = vectorizer.fit_transform(text)\n",
    "    return pd.DataFrame(vector.toarray()), vectorizer\n",
    "\n",
    "tfidf, vectorizer = get_tfidf(jd_data, 'description_jieba_filter')\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tfidf_pca \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tfidf_pca)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tfidf_pca\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tfidf_pca \u001b[39m=\u001b[39m get_tfidf_pca(tfidf, \u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tfidf_pca\n",
      "\u001b[1;32m/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb Cell 25\u001b[0m in \u001b[0;36mget_tfidf_pca\u001b[0;34m(tfidf, n)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tfidf_pca\u001b[39m(tfidf, n\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39mn)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     tfidf_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(tfidf)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tfidf_pca \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tfidf_pca)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jinrirencai/Desktop/golden_data_from_github/golden_data/thu_text.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tfidf_pca\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:407\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    386\u001b[0m     \u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    408\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[1;32m    410\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[1;32m    411\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:459\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_full(X, n_components)\n\u001b[1;32m    458\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_truncated(X, n_components, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_svd_solver)\n\u001b[1;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    462\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnrecognized svd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver)\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:580\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    576\u001b[0m     U, Vt \u001b[39m=\u001b[39m svd_flip(U[:, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], Vt[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    578\u001b[0m \u001b[39melif\u001b[39;00m svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    579\u001b[0m     \u001b[39m# sign flipping is done inside\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m randomized_svd(\n\u001b[1;32m    581\u001b[0m         X,\n\u001b[1;32m    582\u001b[0m         n_components\u001b[39m=\u001b[39;49mn_components,\n\u001b[1;32m    583\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterated_power,\n\u001b[1;32m    584\u001b[0m         flip_sign\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    585\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    586\u001b[0m     )\n\u001b[1;32m    588\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_ \u001b[39m=\u001b[39m n_samples, n_features\n\u001b[1;32m    589\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m Vt\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py:395\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m transpose:\n\u001b[1;32m    392\u001b[0m     \u001b[39m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     M \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 395\u001b[0m Q \u001b[39m=\u001b[39m randomized_range_finder(\n\u001b[1;32m    396\u001b[0m     M,\n\u001b[1;32m    397\u001b[0m     size\u001b[39m=\u001b[39;49mn_random,\n\u001b[1;32m    398\u001b[0m     n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[1;32m    399\u001b[0m     power_iteration_normalizer\u001b[39m=\u001b[39;49mpower_iteration_normalizer,\n\u001b[1;32m    400\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    401\u001b[0m )\n\u001b[1;32m    403\u001b[0m \u001b[39m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[1;32m    404\u001b[0m B \u001b[39m=\u001b[39m safe_sparse_dot(Q\u001b[39m.\u001b[39mT, M)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py:237\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    235\u001b[0m     Q \u001b[39m=\u001b[39m safe_sparse_dot(A\u001b[39m.\u001b[39mT, Q)\n\u001b[1;32m    236\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLU\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 237\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    238\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A\u001b[39m.\u001b[39mT, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQR\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py:153\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    156\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    157\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    158\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    159\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m ):\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# step5：将tfidf降维到n维\n",
    "\n",
    "def get_tfidf_pca(tfidf, n=20):\n",
    "    pca = PCA(n_components=n)\n",
    "    tfidf_pca = pca.fit_transform(tfidf)\n",
    "    tfidf_pca = pd.DataFrame(tfidf_pca)\n",
    "    return tfidf_pca\n",
    "\n",
    "tfidf_pca = get_tfidf_pca(tfidf, 10)\n",
    "tfidf_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f63f3fa61251d3530929b90a6fd0ac6d4fd28461cb09a64c4fa1d09fc6d068be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
